4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 1 of 19https://machinelearningmastery.com/the-transformer-model/NavigationNavigation
Click to Take the FREE Attention Machine Learning Crash-CourseSearch...The Transformer Modelby Stefania Cristina on September 18, 2022 in AttentionLast Updated on January 6, 2023We have already familiarized ourselves with the concept of self-attention as implemented by theTransformer attention mechanism for neural machine translation. We will now be shifting our focusto the details of the Transformer architecture itself to discover how self-attention can beimplemented without relying on the use of recurrence and convolutions.In this tutorial, you will discover the network architecture of the Transformer model.After completing this tutorial, you will know:How the Transformer architecture implements an encoder-decoder structure without recurrenceand convolutions How the Transformer encoder and decoder work How the Transformer self-attention compares to the use of recurrent and convolutional layers Kick-start your project with my book Building Transformer Models with Attention. It provides self-study tutorials with working code to guide you into building a fully-working transformer model thatcantranslate sentences from one language to another...Let’s get started. TweetTweet ShareShareShare4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 2 of 19https://machinelearningmastery.com/the-transformer-model/
The Transformer Model Photo by Samule Sun, some rights reserved.Tutorial OverviewThis tutorial is divided into three parts; they are:The Transformer ArchitectureThe EncoderThe DecoderSum Up: The Transformer ModelComparison to Recurrent and Convolutional LayersNever miss a tutorial:
     
     
     
     
Picked for you:Adding a Custom Attention Layer to aRecurrent Neural Network in KerasTraining the Transformer ModelA Gentle Introduction to PositionalEncoding in Transformer Models, Part 1The Attention Mechanism from ScratchBuilding Transformer Models withAttention Crash Course. Build a Neural4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 3 of 19https://machinelearningmastery.com/the-transformer-model/
PrerequisitesFor this tutorial, we assume that you are already familiar with:The concept of attentionThe attention mechanismThe Transformer attention mechanismThe Transformer ArchitectureThe Transformer architecture follows an encoder-decoder structure but does not rely on recurrenceand convolutions in order to generate an output. 
The  encoder-decoder structure of the Transformer architecture Taken from “Attention Is All You Need“In a nutshell, the task of the encoder, on the left half of the Transformer architecture, is to map aninput sequence to a sequence of continuous representations, which is then fed into a decoder. Machine Translator in 12 Days
×4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 4 of 19https://machinelearningmastery.com/the-transformer-model/
The decoder, on the right half of the architecture, receives the output of the encoder together withthe decoder output at the previous time step to generate an output sequence.
The Encoder
The encoder block of the Transformer architecture Taken from “Attention Is All You Need“At each step the model is auto-regressive, consuming the previously generated symbolsas additional input when generating the next.– Attention Is All You Need, 2017.4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 5 of 19https://machinelearningmastery.com/the-transformer-model/The encoder consists of a stack of  = 6 identical layers, where each layer is composed of twosublayers:1. The ﬁrst sublayer implements a multi-head self-attention mechanism. You have seen that themulti-head mechanism implements  heads that receive a (diﬀerent) linearly projected versionof the queries, keys, and values, each to produce  outputs in parallel that are then used togenerate a ﬁnal result. 2. The second sublayer is a fully connected feed-forward network consisting of two lineartransformations with Rectiﬁed Linear Unit (ReLU) activation in between:The six layers of the Transformer encoder apply the same linear transformations to all the words inthe input sequence, but each layer employs diﬀerent weight () and bias () parametersto do so. Furthermore, each of these two sublayers has a residual connection around it.Each sublayer is also succeeded by a normalization layer, , which normalizes thesum computed between the sublayer input, , and the output generated by the sublayer itself, :An important consideration to keep in mind is that the Transformer architecture cannot inherentlycapture any information about the relative positions of the words in the sequence since it does notmake use of recurrence. This information has to be injected by introducing positional encodings tothe input embeddings. The positional encoding vectors are of the same dimension as the input embeddings and aregenerated using sine and cosine functions of diﬀerent frequencies. Then, they are simply summedto the input embeddings in order to inject the positional information.The Decoder NhhFFN(x)=ReLU(W1x+b1)W2+b2W1,W2b1,b2layernorm(.)xsublayer(x)layernorm(x+sublayer(x))4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 6 of 19https://machinelearningmastery.com/the-transformer-model/
The decoder block of the Transformer architecture Taken from “Attention Is All You Need“The decoder shares several similarities with the encoder. The decoder also consists of a stack of  = 6 identical layers that are each composed of threesublayers:1. The ﬁrst sublayer receives the previous output of the decoder stack, augments it with positionalinformation, and implements multi-head self-attention over it. While the encoder is designed toattend to all words in the input sequence regardless of their position in the sequence, thedecoder is modiﬁed to attend only to the preceding words. Hence, the prediction for a word atposition  can only depend on the known outputs for the words that come before it in thesequence. In the multi-head attention mechanism (which implements multiple, single attentionfunctions in parallel), this is achieved by introducing a mask over the values produced by thescaled multiplication of matrices  and . This masking is implemented by suppressing thematrix values that would otherwise correspond to illegal connections:NiQK/uni239C/uni23A2/uni23A5/uni239F/uni23A2/uni23A54/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 7 of 19https://machinelearningmastery.com/the-transformer-model/
 
The multi-head attention in thedecoder implements severalmasked, single-attention functions Taken from “Attention Is All YouNeed“2. The second layer implements a multi-head self-attention mechanism similar to the oneimplemented in the ﬁrst sublayer of the encoder. On the decoder side, this multi-headmechanism receives the queries from the previous decoder sublayer and the keys and valuesfrom the output of the encoder. This allows the decoder to attend to all the words in the inputsequence.3. The third layer implements a fully connected feed-forward network, similar to the oneimplemented in the second sublayer of the encoder.Furthermore, the three sublayers on the decoder side also have residual connections around themand are succeeded by a normalization layer.mask(QKT)=mask=/uni239B/uni239C/uni239D/uni23A1/uni23A2/uni23A3e11e12…e1ne21e22…e2n/uni22EE/uni22EE/uni22F1/uni22EEem1em2…emn/uni23A4/uni23A5/uni23A6/uni239E/uni239F/uni23A0/uni23A1/uni23A2/uni23A3e11−∞…−∞e21e22…−∞/uni22EE/uni22EE/uni22F1/uni22EEem1em2…emn/uni23A4/uni23A5/uni23A6
The masking makes the decoder unidirectional (unlike the bidirectional encoder).–  Advanced Deep Learning with Python, 2019.4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 8 of 19https://machinelearningmastery.com/the-transformer-model/Positional encodings are also added to the input embeddings of the decoder in the same manner aspreviously explained for the encoder. Want to Get Started With Building Transformer Models with Attention?Take my free 12-day email crash course now (with sample code).Click to sign-up and also get a free PDF Ebook version of the course.Download Your FREE Mini-CourseSum Up: The Transformer ModelThe Transformer model runs as follows:1. Each word forming an input sequence is transformed into a -dimensional embeddingvector. 2. Each embedding vector representing an input word is augmented by summing it (element-wise)to a positional encoding vector of the same  length, hence introducing positionalinformation into the input. 3. The augmented embedding vectors are fed into the encoder block consisting of the twosublayers explained above. Since the encoder attends to all words in the input sequence,irrespective if they precede or succeed the word under consideration, then the Transformerencoder is bidirectional. 4. The decoder receives as input its own predicted output word at time-step, .5. The input to the decoder is also augmented by positional encoding in the same manner doneon the encoder side. 6. The augmented decoder input is fed into the three sublayers comprising the decoder blockexplained above. Masking is applied in the ﬁrst sublayer in order to stop the decoder from/uni239C/uni23A2/uni23A5/uni239F/uni23A2/uni23A5
dmodeldmodelt–14/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 9 of 19https://machinelearningmastery.com/the-transformer-model/attending to the succeeding words. At the second sublayer, the decoder also receives theoutput of the encoder, which now allows the decoder to attend to all the words in the inputsequence.7. The output of the decoder ﬁnally passes through a fully connected layer, followed by a softmaxlayer, to generate a prediction for the next word of the output sequence. Comparison to Recurrent and Convolutional LayersVaswani et al. (2017) explain that their motivation for abandoning the use of recurrence andconvolutions was based on several factors:1. Self-attention layers were found to be faster than recurrent layers for shorter sequence lengthsand can be restricted to consider only a neighborhood in the input sequence for very longsequence lengths. 2. The number of sequential operations required by a recurrent layer is based on the sequencelength, whereas this number remains constant for a self-attention layer. 3. In convolutional neural networks, the kernel width directly aﬀects the long-term dependenciesthat can be established between pairs of input and output positions. Tracking long-termdependencies would require using large kernels or stacks of convolutional layers that couldincrease the computational cost.Further ReadingThis section provides more resources on the topic if you are looking to go deeper.BooksAdvanced Deep Learning with Python, 2019.Papers4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 10 of 19https://machinelearningmastery.com/the-transformer-model/
Attention Is All You Need, 2017.SummaryIn this tutorial, you discovered the network architecture of the Transformer model.Speciﬁcally, you learned:How the Transformer architecture implements an encoder-decoder structure without recurrenceand convolutionsHow the Transformer encoder and decoder workHow the Transformer self-attention compares to recurrent and convolutional layersDo you have any questions?Ask your questions in the comments below, and I will do my best to answer.Learn Transformers and Attention!Teach your deep learning model to read a sentence...using transformer models with attentionDiscover how in my new Ebook:Building Transformer Models with AttentionIt provides self-study tutorials with working code to guide you intobuilding a fully-working transformer models that cantranslate sentences from one language to another...Give magical power of understanding human language forYour Projects
SEE WHAT'S INSIDE
TweetTweet ShareShareShare4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 11 of 19https://machinelearningmastery.com/the-transformer-model/More On This Topic
Building Transformer Models with Attention Crash…
How to Develop a CNN From Scratch for CIFAR-10 Photo…
Implementing the Transformer Decoder from Scratch in…4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 12 of 19https://machinelearningmastery.com/the-transformer-model/ The Transformer Attention MechanismA Gentle Introduction to Positional Encoding in Transformer Models, Part 1 Inferencing the Transformer Model
Implementing the Transformer Encoder from Scratch in…Multi-Label Classiﬁcation of Satellite Photos of…About Stefania CristinaStefania Cristina, PhD is a Lecturer with the Department of Systems and Control Engineering,at the University of Malta.View all posts by Stefania Cristina → attention, machine translation, transformer4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 13 of 19https://machinelearningmastery.com/the-transformer-model/19 Responses to The Transformer ModelKelvin November 17, 2021 at 3:44 am #Hi Stefania Cristina, can you use the encoder block right away after taking convolvedfeatures from a backbone for object detection task excluding the decoder part?
REPLY
Stefania Cristina November 18, 2021 at 7:47 pm #Hi Kelvin, in the computer vision domain it is the encoder block that is usually used,which receives several image patches in parallel, plus the corresponding positional encodings.The encoder output is then typically passed on to an MLP for classiﬁcation. However, I have alsoencountered architectures whereby the encoder block is preceded by convolutional layers,which ﬁrst extract the image features before passing them on to the encoder.
REPLY
Saeid March 31, 2022 at 10:36 pm #Hi Stefania Cristina, Is it possible I use transformer model for time series classiﬁcation? If itis ok, please guide me in coding. I’m a beginner in this ﬁeld.Thanks
REPLY
James Carmichael April 1, 2022 at 9:08 am #Hi Saeid…You may ﬁnd the following of interest:https://towardsdatascience.com/multi-class-classiﬁcation-with-transformers-6cf7b59a033a
REPLY
Furkan Luleci May 13, 2022 at 4:44 am #In the text: “In convolutional neural networks, the kernel width directly aﬀects the long-termdependencies that can be established between pairs of input and output positions. Tracking long-term dependencies would require the use of large kernels, or stacks of convolutional layers thatcould increase the computational cost”I am not sure I get this paragraph correctly. The transformer model gets a sequential input e.g., text,
REPLY4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 14 of 19https://machinelearningmastery.com/the-transformer-model/audio etc. Similarly, tr o use text audio types of input in CNNs, we use 1-D convolutions, which usesingle dimension kernels where the width is always 1. In this case, we only conﬁgure the height ofthe kernel where I mostly use 4 or 7.From my perspective, I think that statement is not a valid comparison.Thanks for the articleStefania Cristina May 13, 2022 at 7:12 pm #Hi Furkan. The idea behind the attention mechanism is to draw global dependenciesbetween the input and the output. This means that the process of predicting an output value willtake into consideration the input data in its entirety (whether this is in the form of text, or animage etc). In this sense we can say that the Transformer can capture long-range dependencies,be it between words when these are distant in lenghty bodies of text, or pixels in distant parts ofan image, etc.In Vaswani et al.’s words, “A single convolutional layer with kernel width k < n does not connectall pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers inthe case of contiguous kernels…”, where n is the sequence length. In CNNs, the receptive ﬁelddepends on the size of the kernel. If one would want to capture long-range dependencies in animage by a CNN, for example, one would either require a large 2D kernel (covering aneighbourhood of k x k pixels) to widen the receptive ﬁeld as much as possible, or stack longsequences of convolutional layers, both of which can be computationally costly. When dealingwith text, you would be working with 1D kernels as you mention, however your kernel width (youreferred to it as height) will still deﬁne the size of your receptive ﬁeld, which is often a localneighbourhood around each token in a sequence.
REPLY
Imene June 28, 2022 at 12:11 am #Thank you so much Sir, it’s really brief and understandable!Please, Could you explain about image classiﬁcation using transformers with codes?
REPLY
James Carmichael June 28, 2022 at 12:50 pm #Hi Imene…The following may be of interest to you:https://paperswithcode.com/method/vision-transformer
REPLY4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 15 of 19https://machinelearningmastery.com/the-transformer-model/Roman Leventov July 19, 2022 at 12:14 am #“The number of sequential operations required by a recurrent layer is based upon thesequence length, whereas this number remains constant for a self-attention layer. ”— This doesn’t make sense, given that the Transformer model is itself limited in the input length?Transformer doesn’t allow to process arbitrarily long sequence with a constant number of operations
!
REPLY
James Carmichael July 19, 2022 at 10:35 am #Hi Roman…The following may be of interest to you:https://machinelearningmastery.com/the-transformer-attention-mechanism/
REPLY
Stefania Cristina July 20, 2022 at 2:41 am #Hi Roman, the comment that you are referring to is not as much about the length of theinput sequence, as it is about the number of operations that are applied to the input. To quoteVaswani et al., “As noted in Table 1, a self-attention layer connects all positions with a constantnumber of sequentially executed operations, whereas a recurrent layer requires O(n) sequentialoperations.”, where n is the input sequence length.Having a look at Table 1 in Vaswani’s paper, we can see that the number of sequential operationsfor the self-attention layer is O(1), which means that it is not a function of the input size, asopposed to the number of sequential operations for recurrent layers. This is because, when theself-attention layer in the Transformer architecture receives its inputs in the form of Queries, Keysand Values, it will apply a set number of sequential operations to them, namely: dot-productmultiplication between the Queries and Keys, followed by scaling (and masking, which ishowever optional), softmax normalisation, and one ﬁnal dot-product multiplication with theValues (refer to the link provided by James for more details).
REPLY
Rashedur Rahman August 19, 2022 at 10:35 pm #Hi Stefania,Thank you very much for the nice and easy to understand article on the transformer model. However,to use transformer as a seq2seq model, I am curious to know how to add constraints to the encoderand/or the decoder. For example, the constraints might be the expected length of the outputsequence. Thanks!
REPLY4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 16 of 19https://machinelearningmastery.com/the-transformer-model/James Carmichael August 20, 2022 at 7:34 am #Hi Rashedur…The following resource may be of interest to you:https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/
REPLY
Rashedur Rahman August 22, 2022 at 6:19 pm #Hi James,Thanks for your reply. In fact, I have an LSTM based seq2seq model to add the constraints (e.g.length of the output sequence). Now I am interested to use transformers (TransformerEncoder andTransformerDecoder) to add the constraint. But I don’t know how to do it with transformers.
REPLY
Hasan October 16, 2022 at 8:31 am #@Stefania Cristina, I have a small doubt.I would be glad if you could answer my followingquestions.@Any response from anyone would be appreciated.Task: “Extractive/Abstractive Text Summarization using NLP with Transformer Model (Ex: T5 basemodel)”1. What are the beneﬁts of having multiple Encoder and Decoder block in Transformers although allencoder and decoder are identical by architecture?2. As very 1st Encoder receives the input sentences in text summarization task and then processedinput vectors transfer to 2nd Encoder and so on. So, what are the changes of these input vectors in2nd to onward Encoders that comes from the 1st Encoder?3. As per the Architecture, Last Encoder’s input vector representations moves to all Decoder, thenwhat are the calculations happen in each Decoder block as all Decoders are in stacked by nature?4. In English to English text summary, which input is fed into the Decoder block?Is it targetsummary? I am bit confused. If target summary is fed into the decoder, then what calculation willmade between targeted summary input and encoder input?5.Which part of the Decoder is basically responsible to choose words/sentences that will be part ofthe ﬁnal summary sentences?It would be really appreciated if anyone can help me to understand these queries.Thanks in Advance.
REPLY4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 17 of 19https://machinelearningmastery.com/the-transformer-model/Leave a Reply
James Carmichael October 17, 2022 at 10:43 am #Hi Hasan…Please narrow your query to one question so that we may better assist you.
REPLYDiego January 22, 2023 at 11:04 pm #hi Stefania,how exactly is the transformer decoder not using for loops to predict its own word based on allprevious predicted words?I see that when training, the decoder iteratively predicts a next word with the ﬁnal dense layer, whichis done in parralel in training but I don’t understand how parrelel training works here.
REPLY
Dario March 3, 2023 at 3:08 am #Questions:– are the input embeddings fed one by one to the encoder?– the identical layers that form the encoder and decoder, are they in parallel or in series (do theyprocess each the same copy of the input, or they are sequentially stacked)?
REPLY
James Carmichael March 3, 2023 at 9:25 am #Hi Dario…The following resource may be of interest:https://machinelearningmastery.com/lstm-autoencoders/
REPLY4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 18 of 19https://machinelearningmastery.com/the-transformer-model/
Name (required)Email (will not be published) (required)
SUBMIT COMMENTWelcome!I'm Jason Brownlee PhD and I help developers get results with machine learning.Read more4/4/23, 6:37 PMThe Transformer Model - MachineLearningMastery.com
Page 19 of 19https://machinelearningmastery.com/the-transformer-model/© 2023 Guiding Tech Media. All Rights Reserved.LinkedIn | Twitter | Facebook | Newsletter | RSSPrivacy | Disclaimer | Terms | Contact | Sitemap | Search